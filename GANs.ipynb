{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4044,
     "status": "ok",
     "timestamp": 1606194587170,
     "user": {
      "displayName": "Hojin Mok",
      "photoUrl": "",
      "userId": "00800008926615240683"
     },
     "user_tz": 300
    },
    "id": "oqqdwWW-Ub2d"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import os\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4026,
     "status": "ok",
     "timestamp": 1606194587183,
     "user": {
      "displayName": "Hojin Mok",
      "photoUrl": "",
      "userId": "00800008926615240683"
     },
     "user_tz": 300
    },
    "id": "LnHWodHtU0FL",
    "outputId": "aed84e9d-c863-4e6f-db46-2f0a393c33e4"
   },
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4012,
     "status": "ok",
     "timestamp": 1606194587185,
     "user": {
      "displayName": "Hojin Mok",
      "photoUrl": "",
      "userId": "00800008926615240683"
     },
     "user_tz": 300
    },
    "id": "2BDCw4PDU8Nj",
    "outputId": "ddf869ca-b020-4008-f641-fae983bb36ce"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "  print(torch.cuda.get_device_name(0))\n",
    "else:\n",
    "  print(\"No GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mv5bh2LyWA_C"
   },
   "source": [
    "# **Part 1 - Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Image.open('CATS_DOGS\\\\train\\\\DOG\\\\0.jpg') as im:\n",
    "  display(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'CATS_DOGS\\\\train' #path right before the train test split\n",
    "img_names =[] #creating an empty list to take in all the image names\n",
    "\n",
    "for folder,subfolders,filenames in os.walk(path):\n",
    "  for img in filenames:\n",
    "    img_names.append(folder+'/'+img)\n",
    "\n",
    "print('There are',len(img_names),'images')\n",
    "print(img_names[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_sizes = []\n",
    "rejected = []\n",
    "\n",
    "for item in img_names:\n",
    "  try:\n",
    "    with Image.open(item) as img:\n",
    "      img_sizes.append(img.size)\n",
    "\n",
    "  except:\n",
    "    rejected.append(item)\n",
    "\n",
    "print(len(img_sizes))\n",
    "print(len(rejected))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33264,
     "status": "ok",
     "timestamp": 1606194616451,
     "user": {
      "displayName": "Hojin Mok",
      "photoUrl": "",
      "userId": "00800008926615240683"
     },
     "user_tz": 300
    },
    "id": "BJvpjk96U_Ni",
    "outputId": "1bbc01a1-d614-4304-a4aa-a7808a3038e6"
   },
   "outputs": [],
   "source": [
    "# Setting some hyperparameters\n",
    "batchSize = 64 # We set the size of the batch.\n",
    "imageSize = 64 # We set the size of the generated images (64x64).\n",
    "\n",
    "# Creating the transformations\n",
    "transform = transforms.Compose([transforms.Resize((imageSize,imageSize)),\n",
    "                                transforms.ToTensor(), \n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                ]) # We create a list of transformations (scaling, tensor conversion, normalization) to apply to the input images.\n",
    "\n",
    "# Loading the dataset\n",
    "dataset = datasets.ImageFolder(root = 'CATS_DOGS\\\\both', transform = transform) #have both the training and testing data in one folder to feed our Discriminator\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size = batchSize, shuffle = True, num_workers = 2) # We use dataLoader to get the images of the training set batch by batch. So now we're loading in the data from the CIFAR training Dataset, and we only load it in in batches of 10 rows at a time out of the 60000 images, with the rows being shuffled (random). Num_workers means how many subprocesses to use for data. so num_workers = 2 means we'll have 2 a parallel threads that will load the data, whereas num_workers=0 means that the data will be loaded in the main process. Use pin_memory = True if using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 33260,
     "status": "ok",
     "timestamp": 1606194616453,
     "user": {
      "displayName": "Hojin Mok",
      "photoUrl": "",
      "userId": "00800008926615240683"
     },
     "user_tz": 300
    },
    "id": "KXd2rnKOEKuY"
   },
   "outputs": [],
   "source": [
    "# Defining the weights_init function that takes as input a neural network m and that will initialize all its weights.\n",
    "def weights_init(model):\n",
    "    classname = model.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        model.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        model.weight.data.normal_(1.0, 0.02)\n",
    "        model.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "po6AH9DVZUG6"
   },
   "source": [
    "# **Part 2 - Creating the Generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 33257,
     "status": "ok",
     "timestamp": 1606194616455,
     "user": {
      "displayName": "Hojin Mok",
      "photoUrl": "",
      "userId": "00800008926615240683"
     },
     "user_tz": 300
    },
    "id": "rvcX8cumY7x8"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() #super is a function that allows us to use Module's functions and variables, while also optimizing this inheritance process\n",
    "        \n",
    "        #Essentially, Sequential() sets up a sequence of neural network layers where each level can include Linear functions, activation functions, pooling layers, convolutional layers, drop out layers, etc. Using sequential combines the layers from a layers list so that it can be ordered properly as a NN, and it is useful for quickly adding in layers and activation functions. Modules will be added to it in the order they are passed in the constructor.\n",
    "        #First we start with an inverse convolution (ConvTranspose2d), since the role of a generator is to generate fake images. Therefore since a CNN takes as images and returns an output vector classifying it, an inverse CNN will do the opposite by taking inputs a classifying vector and will create an image out of it. \n",
    "        #Second, we normalize the 512 feature maps which were outputed from the ConvTranspose2D\n",
    "        #Third we apply the Rectified Linear Unit activation function\n",
    "        #Repeat the 3 above steps until we output the 3 colour channels of the final, fake image\n",
    "        #We finished with a hyperbolic tangent activation function, so that we don't break linearity, and so our values are between -1 to 1 (that's what the asymptopes of a hyperbolic tangent function is). We do this because it will then follow the same standard as the images in the dataset, since the created images of the Generator will be the inputs of the Discriminator along with the CIFAR10 photos. \n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels = 100, out_channels = 512, kernel_size = 4, stride = 1, padding = 0, bias = False),\n",
    "            nn.BatchNorm2d(num_features = 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(in_channels = 512, out_channels = 256, kernel_size = 4, stride = 2, padding = 1, bias = False),\n",
    "            nn.BatchNorm2d(num_features = 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(in_channels = 256, out_channels = 128, kernel_size = 4, stride = 2, padding = 1, bias = False),\n",
    "            nn.BatchNorm2d(num_features = 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(in_channels = 128, out_channels = 64, kernel_size = 4, stride = 2, padding = 1, bias = False),\n",
    "            nn.BatchNorm2d(num_features = 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(in_channels = 64, out_channels = 3, kernel_size = 4, stride = 2, padding = 1, bias = False),\n",
    "            nn.Tanh()          \n",
    "            ) \n",
    "    #This next function is to create the forward propagations through our NN\n",
    "    def forward(self, input):\n",
    "      output = self.main(input) #forwarding through our main Sequential function to get the output\n",
    "      return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33525,
     "status": "ok",
     "timestamp": 1606194616738,
     "user": {
      "displayName": "Hojin Mok",
      "photoUrl": "",
      "userId": "00800008926615240683"
     },
     "user_tz": 300
    },
    "id": "8sT1FpyonjbI",
    "outputId": "573c3d04-e433-47dc-d9ca-82404e76ed0b"
   },
   "outputs": [],
   "source": [
    "netG = Generator()\n",
    "netG.apply(weights_init) #initializing the weights of our discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBxiT57L6YjQ"
   },
   "source": [
    "# **Part 3 - Creating the Discriminator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 33522,
     "status": "ok",
     "timestamp": 1606194616740,
     "user": {
      "displayName": "Hojin Mok",
      "photoUrl": "",
      "userId": "00800008926615240683"
     },
     "user_tz": 300
    },
    "id": "trWQ2irSn-7N"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() #super is a function that allows us to use Module's functions and variables, while also optimizing this inheritance process\n",
    "        \n",
    "        #Essentially, Sequential() sets up a sequence of neural network layers where each level can include Linear functions, activation functions, pooling layers, convolutional layers, drop out layers, etc. Using sequential combines the layers from a layers list so that it can be ordered properly as a NN, and it is useful for quickly adding in layers and activation functions. Modules will be added to it in the order they are passed in the constructor.\n",
    "        #First we start with an convolution (Conv2d), since the role of a Discriminator is to take the images of real objects and output images from the Geneator, and distinguist them. As such, the parameters of the Generator will be the input of the Discriminator. Therefore, the CNN takes in images and returns an output vector classifying it\n",
    "        #Second, we're using a Leaky ReLU as the activation function, with a negative slope of 0.2 (found thru experimentation), as it showed to perform better than a regular ReLU\n",
    "        #Third we make the next convolutional layer\n",
    "        #Fourth, we now normalize the 128 feature maps this time\n",
    "        #Fifth, use the Leaky ReLU again\n",
    "        #Repeat Steps 3 to 5 above steps until we output the 1, which will do the classifying\n",
    "        #We finished with a sigmoid activation function, so that we don't break linearity, and our value will be either 0 or 1 to do the classification\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size = 4, stride = 2, padding = 1, bias = False),\n",
    "            nn.LeakyReLU(negative_slope = 0.2, inplace = True),\n",
    "            nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 4, stride = 2, padding = 1, bias = False),\n",
    "            nn.BatchNorm2d(num_features = 128),\n",
    "            nn.LeakyReLU(negative_slope = 0.2, inplace = True),\n",
    "            nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 4, stride = 2, padding = 1, bias = False),\n",
    "            nn.BatchNorm2d(num_features = 256),\n",
    "            nn.LeakyReLU(negative_slope = 0.2, inplace = True),\n",
    "            nn.Conv2d(in_channels = 256, out_channels = 512, kernel_size = 4, stride = 2, padding = 1, bias = False),\n",
    "            nn.BatchNorm2d(num_features = 512),\n",
    "            nn.LeakyReLU(negative_slope = 0.2, inplace = True),\n",
    "            nn.Conv2d(in_channels = 512, out_channels = 1, kernel_size = 4, stride = 1, padding = 0, bias = False),\n",
    "            nn.Sigmoid()       \n",
    "            ) \n",
    "        \n",
    "    #This next function is to create the forward propagations through our NN\n",
    "    def forward(self, input):\n",
    "      output = self.main(input) #forwarding through our main Sequential function to get the output\n",
    "      return output.view(-1) #so recall we have to actually flatten the CNN before feeding it back into our Generator so it can compare the loss. So here, we're just taking the CNN which is 2 dimensions (cuz it's an image), and flattening it into 1 dimension vector using -1 which infers. This flattened result will complement the batch size, as ANNs and the Generator takes information in batches and it's dimensions will be (batch_size, flattened_CNN_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33511,
     "status": "ok",
     "timestamp": 1606194616742,
     "user": {
      "displayName": "Hojin Mok",
      "photoUrl": "",
      "userId": "00800008926615240683"
     },
     "user_tz": 300
    },
    "id": "BaxUGhiCB0DD",
    "outputId": "1cf3556d-80f6-4325-f18a-301ec26694da"
   },
   "outputs": [],
   "source": [
    "netD = Discriminator()\n",
    "netD.apply(weights_init) #initializing the weights of our discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRp8_T8wChKX"
   },
   "source": [
    "# **Part 4 - Training the GANs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33658,
     "status": "ok",
     "timestamp": 1606194616903,
     "user": {
      "displayName": "Hojin Mok",
      "photoUrl": "",
      "userId": "00800008926615240683"
     },
     "user_tz": 300
    },
    "id": "LtpPdPTYF0Sb",
    "outputId": "cc122899-be55-4151-f41a-abadde87fada"
   },
   "outputs": [],
   "source": [
    "#example\n",
    "ran = torch.rand(2,3,1,2)\n",
    "random_tensor_ex = (ran)\n",
    "print(random_tensor_ex)\n",
    "random_variable_ex = Variable(ran)\n",
    "print(random_variable_ex)\n",
    "print(random_tensor_ex.size()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 33654,
     "status": "ok",
     "timestamp": 1606194616904,
     "user": {
      "displayName": "Hojin Mok",
      "photoUrl": "",
      "userId": "00800008926615240683"
     },
     "user_tz": 300
    },
    "id": "CXOqCrqxCM28"
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss() #so our loss measurement will be based off Binary Cross Entrpy Loss, since this is a mutually exclusive binary answer (either a yes or no, cannot be both)\n",
    "\n",
    "optimizer_D = torch.optim.Adam(netD.parameters(), lr = 0.0002, betas = (0.5, 0.999)) #model parameters are just the fully connected layers and we are using Adam optimizer to optimize them. Betas are coefficients used for computing running averages of gradient and its square\n",
    "optimizer_G = torch.optim.Adam(netG.parameters(), lr = 0.0002, betas = (0.5, 0.999)) #model parameters are just the fully connected layers and we are using Adam optimizer to optimize them. Betas are coefficients used for computing running averages of gradient and its square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7540093,
     "status": "ok",
     "timestamp": 1606202123359,
     "user": {
      "displayName": "Hojin Mok",
      "photoUrl": "",
      "userId": "00800008926615240683"
     },
     "user_tz": 300
    },
    "id": "A3r3OU_aEwqj",
    "outputId": "08162c30-8978-43b5-b094-b55b6a3927b4"
   },
   "outputs": [],
   "source": [
    "#WITH GPU\n",
    "if torch.cuda.is_available():\n",
    "  netD = netD.cuda()\n",
    "  netG = netG.cuda()\n",
    "\n",
    "  import time #Gonna keep track of how long it takes to train our model\n",
    "\n",
    "  start_time = time.time()\n",
    "\n",
    "  #This for loop trains our NN\n",
    "  for epoch in range(100):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "      #i = batches and enumerate just counts what batch number we're on with i (starting from 0), and basically using tuple unpacking to get the actual image data\n",
    "      #data is essentially 2 elements in a tuple, which are (X_train, y_train) of the real images\n",
    "\n",
    "      #Step 1: Updating the weights of the neural network of the Discriminator\n",
    "      netD.zero_grad() #initialzing the gradient on the model\n",
    "      \n",
    "      #1.1: Training the Discriminator with a real image from the dataset\n",
    "      real, _ = data #doing tuple unpacking of data, but only the first element, as saying _ indicates we don't carea bout the second element. Real is the input batches of data, basically X_train of real images\n",
    "      input = Variable(real) #converting our real images into a torch variable, which consist of the tensor data as well as the gradient. Note that we already converted the image data into a PyTorch Tensor above by using transform\n",
    "      target_real = Variable(torch.ones(input.size()[0])) #target is a torch matrix of all ones, since the answer is always correct = 1 for real images. Grabbing index 0 just shows how many batches there are, since the tensor is (batch_size, tuples within each batch, elements within tuple). The batch size in this case is 64, as made at the beginning. See example below\n",
    "      target_fake = Variable(torch.zeros(input.size()[0]))#target is a torch matrix of all zeros, since the answer is always correct = 0 for fake images. Grabbing index 0 just shows how many batches there are, since the tensor is (batch_size, tuples within each batch, elements within tuple). The batch size in this case is 64, as made at the beginning. See example below\n",
    "      target_gen = Variable(torch.ones(input.size()[0])) #so we plan on comparing the outputs of the Discriminator with the target. As such, this means that all the images that the Discriminator accepted as correct, it thought it's target was 1 even if it's incorrect. Therefore, the Generator will only receive data from the Discriminator that it thought was 1, since that is the image we are trying to re-create properly.\n",
    "\n",
    "      input = input.cuda()\n",
    "      target_real = target_real.cuda()\n",
    "\n",
    "      output = netD.forward(input) #getting the output by forward propagating our netD class\n",
    "      loss_realD = criterion(output, target_real) #calculating the loss error\n",
    "\n",
    "      #1.2: Training the Discriminator with a fake image from the Generator\n",
    "      noise = torch.randn(input.size()[0], 100, 1, 1) #(batch size = 64, number of elements per batch (which is 100, because that's what the input of the Generator is), 1x1 item per element). See example below. Here we're comparing noise to the Discriminator to initialize the learning process\n",
    "      noise = Variable(noise) #converting our nosie into a torch variable, so that it has both the noise data and the gradient\n",
    "\n",
    "      noise = noise.cuda()\n",
    "      target_fake = target_fake.cuda()\n",
    "\n",
    "      fake = netG.forward(noise) # so we took the noise, fed it through our netG to create noise images\n",
    "      output = netD.forward(fake.detach()) #now we're taking the noise images from the Generator and feeding it through our Discriminator to train on if the image is a dog or not. We used .detach() to save memory, since fake is a torch variable meaning it has gradient data. We are simple detaching this gradient data from the Generator, since it is not needed to forward propagate through the Discriminator\n",
    "      loss_fakeD = criterion(output, target_fake) #calculating the loss error\n",
    "\n",
    "      #1.3: Backpropagating the total error\n",
    "      loss_D = loss_realD + loss_fakeD #getting total loss\n",
    "      loss_D.backward() #doing backpropagation off the loss function\n",
    "      optimizer_D.step() #using the Discriminator optimizer for the back propagation. So here we just care about the discriminator, because we're just trying to get it good at telling the difference between dogs and noise\n",
    "\n",
    "\n",
    "      #Step 2: Updating the Weights of the neural network of the Generator\n",
    "      netG.zero_grad() #initialzing the gradient on the model\n",
    "\n",
    "      target_gen = target_gen.cuda()\n",
    "\n",
    "      output = netD.forward(fake) #so here we're not using .detach(), because we want to keep the Generator gradient of the fake images, since we want to use it to train\n",
    "      loss_G = criterion(output, target_gen)\n",
    "      loss_G.backward() #doing backpropagation off the loss function\n",
    "      optimizer_G.step() #using the Generator optimizer for the back propagation\n",
    "\n",
    "\n",
    "      #Step 3: Printing losses and saving the real images/generated images\n",
    "      if i % 700 == 0:\n",
    "        print(f'Epoch: {epoch}    Step: {i}     Loss_D: {loss_D.item():10.8f}   Loss_G: {loss_G.item():10.8f}')\n",
    "\n",
    "      if i % 100 == 0:\n",
    "        vutils.save_image(real, 'Results\\\\real_samples.png', normalize = True) #saving the image every 100 batches using torchvision.utils\n",
    "        fake = netG(noise)\n",
    "        vutils.save_image(fake.data, 'Results\\\\fake_samples_epoch_%03d.png' %(epoch), normalize = True) #saving the image every 100 batches using torchvision.utils\n",
    "\n",
    "  print(f'Training took {(time.time() - start_time)/60} minutes')\n",
    "else:\n",
    "  print('Using CPU, run next cell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NO GPU\n",
    "if torch.cuda.is_available():\n",
    "  print('Using GPU, run cell above')\n",
    "else:\n",
    "  import time #Gonna keep track of how long it takes to train our model\n",
    "\n",
    "  start_time = time.time()\n",
    "\n",
    "  #This for loop trains our NN\n",
    "  #This for loop trains our NN\n",
    "  for epoch in range(480,500):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "      #i = batches and enumerate just counts what batch number we're on with i (starting from 0), and basically using tuple unpacking to get the actual image data\n",
    "      #data is essentially 2 elements in a tuple, which are (X_train, y_train) of the real images\n",
    "\n",
    "      #Step 1: Updating the weights of the neural network of the Discriminator\n",
    "      netD.zero_grad() #initialzing the gradient on the model\n",
    "      \n",
    "      #1.1: Training the Discriminator with a real image from the dataset\n",
    "      real, _ = data #doing tuple unpacking of data, but only the first element, as saying _ indicates we don't carea bout the second element. Real is the input batches of data, basically X_train of real images\n",
    "      input = Variable(real) #converting our real images into a torch variable, which consist of the tensor data as well as the gradient. Note that we already converted the image data into a PyTorch Tensor above by using transform\n",
    "      target_real = Variable(torch.ones(input.size()[0])) #target is a torch matrix of all ones, since the answer is always correct = 1 for real images. Grabbing index 0 just shows how many batches there are, since the tensor is (batch_size, tuples within each batch, elements within tuple). The batch size in this case is 64, as made at the beginning. See example below\n",
    "      target_fake = Variable(torch.zeros(input.size()[0]))#target is a torch matrix of all zeros, since the answer is always correct = 0 for fake images. Grabbing index 0 just shows how many batches there are, since the tensor is (batch_size, tuples within each batch, elements within tuple). The batch size in this case is 64, as made at the beginning. See example below\n",
    "      target_gen = Variable(torch.ones(input.size()[0])) #so we plan on comparing the outputs of the Discriminator with the target. As such, this means that all the images that the Discriminator accepted as correct, it thought it's target was 1 even if it's incorrect. Therefore, the Generator will only receive data from the Discriminator that it thought was 1, since that is the image we are trying to re-create properly.\n",
    "\n",
    "      output = netD.forward(input) #getting the output by forward propagating our netD class\n",
    "      loss_realD = criterion(output, target_real) #calculating the loss error\n",
    "\n",
    "      #1.2: Training the Discriminator with a fake image from the Generator\n",
    "      noise = torch.randn(input.size()[0], 100, 1, 1) #(batch size = 64, number of elements per batch (which is 100, because that's what the input of the Generator is), 1x1 item per element). See example below. Here we're comparing noise to the Discriminator to initialize the learning process\n",
    "      noise = Variable(noise) #converting our nosie into a torch variable, so that it has both the noise data and the gradient\n",
    "\n",
    "      fake = netG.forward(noise) # so we took the noise, fed it through our netG to create noise images\n",
    "      output = netD.forward(fake.detach()) #now we're taking the noise images from the Generator and feeding it through our Discriminator to train on if the image is a dog or not. We used .detach() to save memory, since fake is a torch variable meaning it has gradient data. We are simple detaching this gradient data from the Generator, since it is not needed to forward propagate through the Discriminator\n",
    "      loss_fakeD = criterion(output, target_fake) #calculating the loss error\n",
    "\n",
    "      #1.3: Backpropagating the total error\n",
    "      loss_D = loss_realD + loss_fakeD #getting total loss\n",
    "      loss_D.backward() #doing backpropagation off the loss function\n",
    "      optimizer_D.step() #using the Discriminator optimizer for the back propagation. So here we just care about the discriminator, because we're just trying to get it good at telling the difference between dogs and noise\n",
    "\n",
    "\n",
    "      #Step 2: Updating the Weights of the neural network of the Generator\n",
    "      netG.zero_grad() #initialzing the gradient on the model\n",
    "\n",
    "      output = netD.forward(fake) #so here we're not using .detach(), because we want to keep the Generator gradient of the fake images, since we want to use it to train\n",
    "      loss_G = criterion(output, target_gen)\n",
    "      loss_G.backward() #doing backpropagation off the loss function\n",
    "      optimizer_G.step() #using the Generator optimizer for the back propagation\n",
    "    \n",
    "\n",
    "      #Step 3: Printing losses and saving the real images/generated images\n",
    "      if i % 100 == 0:\n",
    "        print(f'Epoch: {epoch:{20}}    Step: {i:{20}}     Loss_D: {loss_D.item():{30}}   Loss_G: {loss_G.item():{30}}    Time: {(time.time() - start_time)/60:->{30}} minutes')\n",
    "        vutils.save_image(real, 'Results\\\\real_samples.png', normalize = True) #saving the image every 100 batches using torchvision.utils\n",
    "        fake = netG(noise)\n",
    "        vutils.save_image(fake.data, 'Results\\\\fake_samples_epoch_%03d.png' %(epoch), normalize = True) #saving the image every 100 batches using torchvision.utils\n",
    "\n",
    "  print(f'Training took {(time.time() - start_time)/60} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(netD.state_dict(), 'Discriminator.pt')\n",
    "torch.save(netG.state_dict(), 'Generator.pt')\n",
    "torch.save(netD.state_dict(),'Discriminator.net')\n",
    "torch.save(netD.state_dict(),'Generator.net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP/QYD61QP4Nkegtj31yvGf",
   "collapsed_sections": [],
   "mount_file_id": "1M_vTjlEaPXObs2uhR24QRQNfKfVf_U96",
   "name": "GANs.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
